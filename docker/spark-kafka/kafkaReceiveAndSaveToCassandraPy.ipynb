{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaReceiveDataPy\n",
    "This notebook receives data from Kafka on the topic 'test', and stores it in the 'time_test' table of Cassandra (created by cassandra_init.script in startup_script.sh).\n",
    "\n",
    "```\n",
    "CREATE KEYSPACE test_time WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "\n",
    "CREATE TABLE test_time.sent_received(\n",
    " time_sent TEXT,\n",
    " time_received TEXT,\n",
    "PRIMARY KEY (time_sent)\n",
    ");\n",
    "```\n",
    "\n",
    "A message that gives the current time is received every second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and start SparkContext\n",
    "Note that SparkContext must be started to effectively load the package dependencies. Two cores are used, since one is needed for running the Kafka receiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Streaming test\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveToCassandra function\n",
    "Takes a list of tuple (rows) and save to Cassandra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveToCassandra(rows):\n",
    "    if not rows.isEmpty(): \n",
    "        sqlContext.createDataFrame(rows).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=\"sent_received\", keyspace=\"test_time\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create streaming task\n",
    "* Receive data from Kafka 'test' topic every five seconds\n",
    "* Get stream content, and add receiving time to each message\n",
    "* Save each RDD in the DStream to Cassandra. Also print on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5)\n",
    "kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {'test': 1})\n",
    "data = kvs.map(lambda x: x[1])\n",
    "rows= data.map(lambda x:Row(time_sent=x,time_received=time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "rows.foreachRDD(saveToCassandra)\n",
    "rows.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:22:30\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:45')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:46')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:47')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:48')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:49')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:50')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:51')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:52')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:53')\n",
      "Row(time_received='2018-12-08 08:22:37', time_sent=u'2018-12-08 06:38:54')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:22:35\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:22:38', time_sent=u'2018-12-08 08:22:30')\n",
      "Row(time_received='2018-12-08 08:22:38', time_sent=u'2018-12-08 08:22:31')\n",
      "Row(time_received='2018-12-08 08:22:38', time_sent=u'2018-12-08 08:22:32')\n",
      "Row(time_received='2018-12-08 08:22:38', time_sent=u'2018-12-08 08:22:33')\n",
      "Row(time_received='2018-12-08 08:22:38', time_sent=u'2018-12-08 08:22:34')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:22:40\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:22:40', time_sent=u'2018-12-08 08:22:35')\n",
      "Row(time_received='2018-12-08 08:22:40', time_sent=u'2018-12-08 08:22:36')\n",
      "Row(time_received='2018-12-08 08:22:40', time_sent=u'2018-12-08 08:22:37')\n",
      "Row(time_received='2018-12-08 08:22:41', time_sent=u'2018-12-08 08:22:38')\n",
      "Row(time_received='2018-12-08 08:22:41', time_sent=u'2018-12-08 08:22:39')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:22:45\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:22:45', time_sent=u'2018-12-08 08:22:40')\n",
      "Row(time_received='2018-12-08 08:22:45', time_sent=u'2018-12-08 08:22:41')\n",
      "Row(time_received='2018-12-08 08:22:45', time_sent=u'2018-12-08 08:22:42')\n",
      "Row(time_received='2018-12-08 08:22:46', time_sent=u'2018-12-08 08:22:43')\n",
      "Row(time_received='2018-12-08 08:22:46', time_sent=u'2018-12-08 08:22:44')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:22:50\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:22:50', time_sent=u'2018-12-08 08:22:45')\n",
      "Row(time_received='2018-12-08 08:22:50', time_sent=u'2018-12-08 08:22:46')\n",
      "Row(time_received='2018-12-08 08:22:50', time_sent=u'2018-12-08 08:22:47')\n",
      "Row(time_received='2018-12-08 08:22:51', time_sent=u'2018-12-08 08:22:48')\n",
      "Row(time_received='2018-12-08 08:22:51', time_sent=u'2018-12-08 08:22:49')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:22:55\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:22:55', time_sent=u'2018-12-08 08:22:50')\n",
      "Row(time_received='2018-12-08 08:22:55', time_sent=u'2018-12-08 08:22:51')\n",
      "Row(time_received='2018-12-08 08:22:55', time_sent=u'2018-12-08 08:22:52')\n",
      "Row(time_received='2018-12-08 08:22:56', time_sent=u'2018-12-08 08:22:53')\n",
      "Row(time_received='2018-12-08 08:22:56', time_sent=u'2018-12-08 08:22:54')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:23:00\n",
      "-------------------------------------------\n",
      "Row(time_received='2018-12-08 08:23:00', time_sent=u'2018-12-08 08:22:55')\n",
      "Row(time_received='2018-12-08 08:23:00', time_sent=u'2018-12-08 08:22:56')\n",
      "Row(time_received='2018-12-08 08:23:00', time_sent=u'2018-12-08 08:22:57')\n",
      "Row(time_received='2018-12-08 08:23:00', time_sent=u'2018-12-08 08:22:58')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:23:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-12-08 08:23:10\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|          time_sent|      time_received|\n",
      "+-------------------+-------------------+\n",
      "|2018-12-08 07:30:53|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:16:44|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:16:18|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:15:36|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:17:39|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:15:50|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:07:31|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:35:54|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:29:54|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:57:38|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:00:06|2018-12-08 08:22:33|\n",
      "|2018-12-08 06:46:27|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:29:24|2018-12-08 08:22:33|\n",
      "|2018-12-08 06:49:03|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:40:23|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:45:06|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:22:21|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:43:29|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:26:38|2018-12-08 08:22:33|\n",
      "|2018-12-08 06:44:04|2018-12-08 08:22:33|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=sqlContext.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"sent_received\", keyspace=\"test_time\")\\\n",
    "    .load()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_sent: string (nullable = true)\n",
      " |-- time_received: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|          time_sent|      time_received|\n",
      "+-------------------+-------------------+\n",
      "|2018-12-08 07:30:53|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:16:44|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:16:18|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:15:36|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:17:39|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:15:50|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:07:31|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:35:54|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:29:54|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:57:38|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:00:06|2018-12-08 08:22:33|\n",
      "|2018-12-08 06:46:27|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:29:24|2018-12-08 08:22:33|\n",
      "|2018-12-08 06:49:03|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:40:23|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:45:06|2018-12-08 08:22:33|\n",
      "|2018-12-08 08:22:21|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:43:29|2018-12-08 08:22:33|\n",
      "|2018-12-08 07:26:38|2018-12-08 08:22:33|\n",
      "|2018-12-08 06:44:04|2018-12-08 08:22:33|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable(\"sent_received\");\n",
    "data.printSchema()\n",
    "data=sqlContext.sql(\"select * from sent_received\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson - Processing Streamed Data With PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been saved to a Spark DataFrame, we can begin to process the data through a pipeline.\n",
    "\n",
    "Dates, times, and timestamps are notoriously difficult to work with in Python, even within Pandas! However, working with them in Spark proves to be fairly easy.\n",
    "\n",
    "First, we'll import `pyspark.sql.functions`. This gives you access to common SQL functions within PySpark, and are useful for processing data beyond what's shown below.\n",
    "\n",
    "We'll first define `split_col`, which will split the `time_sent` column at the whitespace between date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "split_time_sent = f.split(data['time_sent'], ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining `split_col`, we can separate the date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stream = data.withColumn('date_time_sent', split_time_sent.getItem(0))\n",
    "df_stream = df_stream.withColumn('time_time_sent', split_time_sent.getItem(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now perform the same operation on the `time_received` column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_time = f.split(df_stream['time_received'], ' ')\n",
    "\n",
    "df_stream = df_stream.withColumn('date_time_received', split_time.getItem(0))\n",
    "df_stream = df_stream.withColumn('time_time_received', split_time.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------+--------------+------------------+------------------+\n",
      "|          time_sent|      time_received|date_time_sent|time_time_sent|date_time_received|time_time_received|\n",
      "+-------------------+-------------------+--------------+--------------+------------------+------------------+\n",
      "|2018-12-08 07:30:53|2018-12-08 08:22:33|    2018-12-08|      07:30:53|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:16:44|2018-12-08 08:22:33|    2018-12-08|      08:16:44|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:16:18|2018-12-08 08:22:33|    2018-12-08|      08:16:18|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:15:36|2018-12-08 08:22:33|    2018-12-08|      08:15:36|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:17:39|2018-12-08 08:22:33|    2018-12-08|      07:17:39|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:15:50|2018-12-08 08:22:33|    2018-12-08|      07:15:50|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:07:31|2018-12-08 08:22:33|    2018-12-08|      07:07:31|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:35:54|2018-12-08 08:22:33|    2018-12-08|      07:35:54|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:29:54|2018-12-08 08:22:33|    2018-12-08|      07:29:54|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:57:38|2018-12-08 08:22:33|    2018-12-08|      07:57:38|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:00:06|2018-12-08 08:22:33|    2018-12-08|      08:00:06|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 06:46:27|2018-12-08 08:22:33|    2018-12-08|      06:46:27|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:29:24|2018-12-08 08:22:33|    2018-12-08|      07:29:24|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 06:49:03|2018-12-08 08:22:33|    2018-12-08|      06:49:03|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:40:23|2018-12-08 08:22:33|    2018-12-08|      07:40:23|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:45:06|2018-12-08 08:22:33|    2018-12-08|      07:45:06|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:22:21|2018-12-08 08:22:33|    2018-12-08|      08:22:21|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:43:29|2018-12-08 08:22:33|    2018-12-08|      07:43:29|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:26:38|2018-12-08 08:22:33|    2018-12-08|      07:26:38|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 06:44:04|2018-12-08 08:22:33|    2018-12-08|      06:44:04|        2018-12-08|          08:22:33|\n",
      "+-------------------+-------------------+--------------+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema to look at the data types of `dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_sent: string (nullable = true)\n",
      " |-- time_received: string (nullable = true)\n",
      " |-- date_time_sent: string (nullable = true)\n",
      " |-- time_time_sent: string (nullable = true)\n",
      " |-- date_time_received: string (nullable = true)\n",
      " |-- time_time_received: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this output from Kafka is a string representation, we'll need to convert each column we created to a relevant data type.\n",
    "\n",
    "First, convert the original `time_sent` and `time_received` columns from `string` to `timestamp`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stream = df_stream.withColumn(\"time_sent\", f.to_utc_timestamp(\"time_sent\", \"dd-MMM-yy\"))\n",
    "df_stream = df_stream.withColumn(\"time_received\", f.to_utc_timestamp(\"time_received\", \"dd-MMM-yy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema, and show the data again. Notice that `time_sent` and `time_received` are now converted to a `timestamp` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_sent: timestamp (nullable = true)\n",
      " |-- time_received: timestamp (nullable = true)\n",
      " |-- date_time_sent: string (nullable = true)\n",
      " |-- time_time_sent: string (nullable = true)\n",
      " |-- date_time_received: string (nullable = true)\n",
      " |-- time_time_received: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------+--------------+------------------+------------------+\n",
      "|           time_sent|       time_received|date_time_sent|time_time_sent|date_time_received|time_time_received|\n",
      "+--------------------+--------------------+--------------+--------------+------------------+------------------+\n",
      "|2018-12-08 07:30:...|2018-12-08 08:22:...|    2018-12-08|      07:30:53|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:16:...|2018-12-08 08:22:...|    2018-12-08|      08:16:44|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:16:...|2018-12-08 08:22:...|    2018-12-08|      08:16:18|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:15:...|2018-12-08 08:22:...|    2018-12-08|      08:15:36|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:17:...|2018-12-08 08:22:...|    2018-12-08|      07:17:39|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:15:...|2018-12-08 08:22:...|    2018-12-08|      07:15:50|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:07:...|2018-12-08 08:22:...|    2018-12-08|      07:07:31|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:35:...|2018-12-08 08:22:...|    2018-12-08|      07:35:54|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:29:...|2018-12-08 08:22:...|    2018-12-08|      07:29:54|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:57:...|2018-12-08 08:22:...|    2018-12-08|      07:57:38|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:00:...|2018-12-08 08:22:...|    2018-12-08|      08:00:06|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 06:46:...|2018-12-08 08:22:...|    2018-12-08|      06:46:27|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:29:...|2018-12-08 08:22:...|    2018-12-08|      07:29:24|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 06:49:...|2018-12-08 08:22:...|    2018-12-08|      06:49:03|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:40:...|2018-12-08 08:22:...|    2018-12-08|      07:40:23|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:45:...|2018-12-08 08:22:...|    2018-12-08|      07:45:06|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 08:22:...|2018-12-08 08:22:...|    2018-12-08|      08:22:21|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:43:...|2018-12-08 08:22:...|    2018-12-08|      07:43:29|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 07:26:...|2018-12-08 08:22:...|    2018-12-08|      07:26:38|        2018-12-08|          08:22:33|\n",
      "|2018-12-08 06:44:...|2018-12-08 08:22:...|    2018-12-08|      06:44:04|        2018-12-08|          08:22:33|\n",
      "+--------------------+--------------------+--------------+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream.printSchema()\n",
    "df_stream.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is processed, let's convert the Spark DataFrame to Pandas & format as JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine Spark DataFrame With Existing CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll often be joining data to existing sources. This example shows how to join an existing CSV with a Spark DataFrame, process the data, and save to JSON.\n",
    "\n",
    "Import the `titanic_train.csv` data into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic = sqlContext.read.csv(\"titanic_train.csv\", header=True, inferSchema=True).limit(df_stream.count())\n",
    "type(df_titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the data with the `show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_titanic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine this data, we're going to create a new, auto-incremented column called `PassengerId` within the `df_stream` DataFrame.\n",
    "\n",
    "After creating the column, we'll convert the data type from `long` to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_stream = df_stream.withColumn(\"PassengerId\", f.monotonically_increasing_id())\n",
    "\n",
    "df_stream = df_stream.withColumn(\"PassengerId\", df_stream[\"PassengerId\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the data into a new Spark DataFrame, `df_combined`.\n",
    "\n",
    "Then, drop the duplicated `PassengerId` column, using the `join` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_stream.join(df_titanic, df_stream.PassengerId == df_titanic.PassengerId).drop(df_stream.PassengerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_sent: timestamp (nullable = true)\n",
      " |-- time_received: timestamp (nullable = true)\n",
      " |-- date_time_sent: string (nullable = true)\n",
      " |-- time_time_sent: string (nullable = true)\n",
      " |-- date_time_received: string (nullable = true)\n",
      " |-- time_time_received: string (nullable = true)\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "('Total Combined Rows: ', 4584)\n"
     ]
    }
   ],
   "source": [
    "df_combined.printSchema()\n",
    "print(\"Total Combined Rows: \", df_combined.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drop a few columns, considering they're redundant, and have already been processed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['time_sent', 'time_received']\n",
    "\n",
    "df_combined = df_combined.select([column for column in df_combined.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the DataFrame schema to make sure the columns have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_time_sent: string (nullable = true)\n",
      " |-- time_time_sent: string (nullable = true)\n",
      " |-- date_time_received: string (nullable = true)\n",
      " |-- time_time_received: string (nullable = true)\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the `Age` and `Cabin` variables. Some have a value of `null`, meaning they're missing.\n",
    "\n",
    "In Spark, we'll _impute_ the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+\n",
      "|PassengerId| Age|Cabin|\n",
      "+-----------+----+-----+\n",
      "|          1|22.0| null|\n",
      "|          2|38.0|  C85|\n",
      "|          3|26.0| null|\n",
      "|          4|35.0| C123|\n",
      "|          5|35.0| null|\n",
      "|          6|null| null|\n",
      "|          7|54.0|  E46|\n",
      "|          8| 2.0| null|\n",
      "|          9|27.0| null|\n",
      "|         10|14.0| null|\n",
      "|         11| 4.0|   G6|\n",
      "|         12|58.0| C103|\n",
      "|         13|20.0| null|\n",
      "|         14|39.0| null|\n",
      "|         15|14.0| null|\n",
      "|         16|55.0| null|\n",
      "|         17| 2.0| null|\n",
      "|         18|null| null|\n",
      "|         19|31.0| null|\n",
      "|         20|null| null|\n",
      "+-----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.select(\"PassengerId\", \"Age\", \"Cabin\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll get the average age of all passengers. But we'll make sure to exclude those who don't have an age, to avoid skewing the average.\n",
    "\n",
    "We'll also fill the `Cabin` column with the value `__NA__`, showing there was no value given in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, col\n",
    "\n",
    "avg_age = df_combined.select(mean(df_combined[\"Age\"]).alias(\"Age\")).where(col(\"Age\").isNotNull())\n",
    "avg_age.rdd.map(lambda row: row.asDict())\n",
    "avg_age = avg_age.collect()[0][\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a sanity check to make sure the values are filled in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+\n",
      "|PassengerId| Age| Cabin|\n",
      "+-----------+----+------+\n",
      "|          1|22.0|__NA__|\n",
      "|          2|38.0|   C85|\n",
      "|          3|26.0|__NA__|\n",
      "|          4|35.0|  C123|\n",
      "|          5|35.0|__NA__|\n",
      "|          6|30.0|__NA__|\n",
      "|          7|54.0|   E46|\n",
      "|          8| 2.0|__NA__|\n",
      "|          9|27.0|__NA__|\n",
      "|         10|14.0|__NA__|\n",
      "|         11| 4.0|    G6|\n",
      "|         12|58.0|  C103|\n",
      "|         13|20.0|__NA__|\n",
      "|         14|39.0|__NA__|\n",
      "|         15|14.0|__NA__|\n",
      "|         16|55.0|__NA__|\n",
      "|         17| 2.0|__NA__|\n",
      "|         18|30.0|__NA__|\n",
      "|         19|31.0|__NA__|\n",
      "|         20|30.0|__NA__|\n",
      "+-----------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.select(\"PassengerId\", \"Age\", \"Cabin\").fillna({'Age': round(avg_age), 'Cabin':\"__NA__\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fill in the values within the `df_combined` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.fillna({'Age': round(avg_age), 'Cabin':\"__NA__\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to JSON "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing our data, we'll want to save the data to a JSON file. \n",
    "\n",
    "However, if you try to save the DataFrame as a JSON object using `toJSON`, you'll find the DataFrame will be saved as a collection of files. \n",
    "\n",
    "Because we're using Spark, our data is spread across multiple nodes, computing in parallel, and saved as parts to a directory. And larger datasets means more files. With Spark, you don't want to save locally; at the end of processing, you'll almost always send the data to a database, or to a storage service like Amazon S3.\n",
    "\n",
    "However, there is a way to get around this: converting a Spark DataFrame to a Pandas DataFrame, then saving as JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_combined = df_combined.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting to Pandas, we'll save the file to JSON on our local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_combined.to_json(\"spark_titanic.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
